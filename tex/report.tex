\documentclass{article}
% \usepackage{sbc-template}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{pgfplots}
\usepackage{pgfplotstable} 
\usepackage{titlesec}
\usepackage{lipsum}
\usepackage{authblk}
\usepackage{mathtools}

\begin{document}

\title{Recuperação de Informação - Máquinas de Busca na Web: Trabalho Prático 3}
\author{João Mateus de Freitas Veneroso}
\affil{Departamento de Ciência da Computação da Universidade Federal de Minas Gerais}

\maketitle

\section{Introdução}

Este trabalho descreve a implementação de um sistema de consultas com base no arquivo
invertido criado no trabalho prático 2. O sistema implementado faz 
consultas ordenadas por meio do modelo de espaço vetorial com a utilização de informação dos
\textit{anchor texts} e do \textit{page rank} para melhoria dos resultados. A interface gráfica do sistema
de consultas foi implementada em NodeJS e está disponível para consultas na
url visconde.latin.dcc.ufmg.br:8080. A versão mais recente do código pode ser obtida
em: https://github.com/jmfveneroso/inverted-index. Para instruções de compilação e uso, leia
o arquivo README.md.

\section{Índice invertido}

Esta seção descreve brevemente os detalhes do índice invertido utilizado pelo sistema
de consultas. As páginas foram reindexadas com o mesmo sistema construído no trabalho
prático 2. A coleção completa tem 6.175.317 páginas da web no domínio ".br" ocupando
aproximadamente 477 GB. 

\begin{table}
\centering
\begin{tabular}{ |l|l| }
  \hline
  \multicolumn{2}{|c|}{Etapa de extração} \\
  \hline
  Tamanho da coleção & 477 GB \\
  Número de documentos & 6.175.317 \\
  Número de termos & 9.273.617 \\
  Tamanho do arquivo invertido & 11.20 GB \\
  Tempo de carregamento & 120 s \\
  \hline
\end{tabular}
\caption{Índice invertido}
\label{tab:inverted_index}
\end{table}

As características do arquivo invertido estão descritas na 
tabela \ref{tab:inverted_index}. As \textit{stop words} do inglês e do 
português não foram indexadas. Além disso, somente palavras entre 3 e 30 caracteres
foram consideradas no índice. As listas invertidas de cada termo são comprimidas com
o método Elias-$ \delta $. Além das listas invertidas, o índice contém a norma dos vetores
dos documentos, a frequência do termo na coleção, o page rank dos documentos
e as listas invertidas dos \textit{anchor texts}. O índice está disponível em:
"/mnt/hd0/joao\_test/inverted\_index\_tp3.data" na máquina visconde.latin.

\section{Sistema de consultas}

O sistema de consultas implementado sempre recupera as listas invertidas completas dos termos
procurados e só considera no ranking os documentos que pertencem à interseção das listas invertidas, 
ou seja, os documentos que contém todos os termos procurados. Para palavras que aparecem em muitos documentos, esse
detalhe de implementação pode onerar um pouco a velocidade da máquina de busca. A menor lista 
invertida é recuperada primeiro e, para cada uma das listas invertidas posteriores, apenas os
documentos que já apareceram na primeira lista invertida são mantidos. 

Antes de realizar as buscas, duas estruturas devem ser carregadas em memória principal: 
o dicionário e o mapa de documentos. O dicionário contém todos os termos, a posição das
suas respectivas listas invertidas no arquivo invertido, a posição das suas listas invertidas
de \textit{anchor texts} e a frequência do termo na coleção. O mapa de documentos guarda, 
para cada documento, a id, a url e o page rank. O carregamento destas estruturas 
demora cerca de 120 segundos para o índice descrito na seção anterior. Após o carregamento, as buscas
demoram apenas alguns milisegundos para termos pouco frequentes, mas podem demorar mais para termos
muito frequentes.

O sistema de consultas utiliza três modelos diferentes para realizar o ordenamento dos documentos
encontrados: o modelo vetorial, o modelo de anchor text e o page rank. Cada um destes modelos
possui um peso associado que multiplica o \textit{score} atribuído a cada documento. Estes pesos
servem para ajustar de forma linear a contribuição de cada modelo para o \textit{ranking} final.

\section{Modelo de espaço vetorial}

O modelo de espaço vetorial é o modelo base do nosso sistema de consultas. A medida que as
listas invertidas são analisadas, um acumulador é mantido para cada um dos documentos 
encontrados. Para cada termo, somamos a seguinte expressão ao acumulador:

\begin{equation}
(1 + log(f_{t,d})) \times log(\frac{N}{n_t})
\label{eq:vector}
\end{equation}

onde $ f_{t,d} $ é a frequência do termo no documento, que obtemos na lista invertida do termo, e
$ \frac{N}{n_t} $ é a frequência inversa do termo na coleção, que obtemos no dicionário. Ao
final da análise de todos os documentos, dividimos cada um dos acumuladores pela norma do
seu respectivo documento, que foi salva no índice durante a sua construção. A norma do documento consiste
em somar a equação \ref{eq:vector} para todos os termos da coleção e extrair
a raiz quadrada do resultado. O cálculo dessa soma é trivial após a construção do índice.

\section{Anchor text}

O modelo de \textit{ranking} pelos \textit{anchor texts} funciona com base nos termos
utilizados no texto dos \textit{links} da coleção. Cada termo possui uma lista invertida análoga ao índice principal.
No entanto, essa lista só contém a referência dos documentos apontados pelo 
termo e um contador para saber quantas vezes o termo apareceu apontando cada documento.
\textit{Links} que apontavam documentos fora da coleção foram ignorados. 
O índice de \textit{anchor ranks} para o arquivo invertido descrito na seção 1 ocupa cerca de 2GB.

O \textit{score} atribuído a cada documento é simplesmente o número de vezes que o termo da busca 
foi encontrado em \textit{anchor texts} de \textit{links} que apontavam para um dado documento. 
Para a maioria dos documentos, o \textit{score} será zero. Pois, o volume de texto nos \textit{anchor texts}
é bastante limitado. No entanto, em vários casos de busca, o \textit{anchor text} se mostrou uma 
ferramenta valiosa, já que ele atua como um título para a página apontada.

\section{Page Rank}

O \textit{Page Rank} é uma forma de atribuir relevância aos documentos com base
no conceito de recomendação entre páginas da web, independente dos termos de busca. 
O \textit{Page Rank} foi calculado junto ao índice de \textit{Anchor Texts} 
em uma segunda passagem sobre a coleção após o índice principal já ter sido construído. Para cada \textit{link}
que apontava para um documento dentro da coleção, foi armazenado um ponteiro da origem para o destino,
formando um grafo de recomendação entre os documentos.

Inicialmente, atribuiu-se o valor 1 para cada documento e o algoritmo de cálculo do \textit{Page Rank} foi executado por 300 iterações.
Um alto grau de convergência pode ser alcançado com um número muito menor de iterações, no entanto, já que a coleção é relativamente
pequena, é viável executar um número maior de iterações para alcançar um grau melhor de convergência.

O procedimento executado em cada iteração foi o seguinte: os documentos de origem distribuem seu \textit{score} para os documentos apontados 
pelos seus links. Documentos \textit{sink} (que não contém \textit{links} de saída) têm seu score acumulado e, ao final da iteração,
esse \textit{score} é distribuído entre todos os documentos da coleção. Por conta da distribuição dos \textit{scores}, 
a soma dos \textit{Page Ranks} ao final da iteração é sempre igual ao número de documentos na coleção. No entanto, documentos com
mais "recomendações" acumulam um \textit{score} maior. 

O valor do \textit{Page Rank} é armazenado no mapa de documentos e pode servir para ordenar os documentos por
relevância. No entanto, os resultados são melhores quando ele é usado em associação com outro método como o modelo
de espaço vetorial. Como a ordem de grandeza dos valores artibuídos pelo \textit{Page Rank} acaba sendo muito maior
do que o peso atribuído pelo modelo de espaço vetorial, é necessário usar um fator de normalização, como explicado
na seção 2. O \textit{Page Rank} não se mostrou um sinal muito eficiente nas consultas de teste, provavelmente por
conta do tamanho limitado da coleção e do viés de seleção das páginas pelos \textit{web crawlers}.

\section{Servidor e interface gráfica}

O servidor da máquina de busca foi implementado em \textit{NodeJS}, um \textit{runtime environment} 
que executa \textit{Javascript} do lado do servidor. Um módulo do \textit{node-gyp} foi construído
para transformar a máquina de busca (C++) em uma biblioteca do \textit{Javascript}. O servidor
utiliza o \textit{framework} Express para responder às requisições http e a linguagem de marcação 
Jade para construir a interface gráfica. 

O servidor responde requisições nas rotas:

\subsection{GET /}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{page_1.png}
\caption{Página inicial com a consulta: "fibrose cística"} 
\label{fig:page_1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{page_1_modal.png}
\caption{Modal de ajuste dos pesos relativos dos modelos de ordenamento} 
\label{fig:page_1_modal}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{doc.png}
\caption{Exemplo de conteúdo de página} 
\label{fig:doc}
\end{figure}

A página inicial conta com uma caixa de texto onde o usuário pode fazer consultas. A figura \ref{fig:page_1}
mostra um exemplo de consulta. 
Um ícone à direita da caixa de consultas abre um modal onde os pesos relativos dos modelos de \textit{ranking}
podem ser ajustados (basta colocar zero no campo para desativar um modelo), como mostrado na figura
\ref{fig:page_1_modal}. A página só mostra 
10 resultados. Para navegar pelas próximas páginas de resultados é necessário clicar nos botões
"Previous Page" e "Next Page" na parte de baixo da página.
Cada resultado mostra o título da página (extraído da tag <title> no html), a url da página 
seguida do \textit{score} e um pequeno trecho onde os termos buscados
ocorrem no documento.

Parâmetros: 
\begin{itemize}
  \item q = consulta
  \item vector = peso do modelo de espaço vetorial (padrão = 1000)
  \item anchor = peso do modelo de anchor texts (padrão = 100)
  \item pagerank = peso do page rank (padrão = 10)
  \item page = página atual da consulta (padrão = 1)
\end{itemize}

\subsection{GET /doc}
A rota /doc mostra o conteúdo de um documento. Como o html extraído pelo crawler pode conter links para
arquivos CSS e Javascript que não existem mais, a apresentação da página pode ficar prejudicada. A 
figura \ref{fig:doc} mostra o conteúdo da primeira página retornada pela consulta "fibrose cística". 

Parâmetros: 
\begin{itemize}
  \item id = id do documento
\end{itemize}

\section{Experimentos}

\begin{table}
\centering
\begin{tabular}{ |l|l|l|l|l| }
  \hline
  \multicolumn{1}{|c|}{Estatística} & VEC & VEC+ANCHOR & VEC+PAGERANK & COMPLETE \\
  \hline
  num\_ret                 & 28574 & 28575 & 28576 & 28572 \\
  num\_rel                 & 247   & 247   & 247   & 247   \\
  num\_rel\_ret            & 126   & 180   & 125   & 178   \\
  map                      & 0.156 & 0.301 & 0.044 & 0.300 \\
  NDCG\_5                  & 0.328 & 0.576 & 0.080 & 0.596 \\
  P\_5                     & 0.252 & 0.413 & 0.039 & 0.413 \\
  P\_10                    & 0.165 & 0.374 & 0.045 & 0.387 \\
  P\_15                    & 0.118 & 0.262 & 0.047 & 0.264 \\
  P\_20                    & 0.094 & 0.198 & 0.040 & 0.205 \\
  P\_30                    & 0.065 & 0.133 & 0.037 & 0.138 \\
  P\_50                    & 0.045 & 0.083 & 0.025 & 0.084 \\
  P\_100                   & 0.028 & 0.045 & 0.020 & 0.045 \\
  P\_200                   & 0.016 & 0.024 & 0.013 & 0.024 \\
  P\_500                   & 0.007 & 0.011 & 0.006 & 0.011 \\
  P\_1000                  & 0.004 & 0.006 & 0.004 & 0.006 \\
  iprec\_at\_recall\_0.00  & 0.461 & 0.589 & 0.097 & 0.580 \\
  iprec\_at\_recall\_0.10  & 0.396 & 0.567 & 0.097 & 0.567 \\
  iprec\_at\_recall\_0.20  & 0.331 & 0.527 & 0.093 & 0.529 \\
  iprec\_at\_recall\_0.30  & 0.236 & 0.472 & 0.071 & 0.473 \\
  iprec\_at\_recall\_0.40  & 0.121 & 0.400 & 0.057 & 0.407 \\
  iprec\_at\_recall\_0.50  & 0.108 & 0.341 & 0.055 & 0.345 \\
  iprec\_at\_recall\_0.60  & 0.084 & 0.298 & 0.038 & 0.300 \\
  iprec\_at\_recall\_0.70  & 0.064 & 0.183 & 0.029 & 0.179 \\
  iprec\_at\_recall\_0.80  & 0.046 & 0.108 & 0.025 & 0.104 \\
  iprec\_at\_recall\_0.90  & 0.017 & 0.064 & 0.010 & 0.058 \\
  iprec\_at\_recall\_1.00  & 0.017 & 0.047 & 0.009 & 0.043 \\
  \hline
\end{tabular}
\caption{Resultados gerais}
\label{tab:general_results}
\end{table}

Os experimentos mediram o desempenho de quatro modelos em 30 consultas. Os modelos testados são: 
modelo vetorial (VEC), modelo vetorial com \textit{anchor texts} (VEC+ANCHOR), 
modelo vetorial com \textit{page rank} (VEC+PAGERANK) e o modelo vetorial com \textit{anchor texts} e \textit{page rank} (COMPLETE).
Antes de analisar os resultados das consultas específicas, a tabela \ref{tab:general_results} descreve
os resultados gerais dos quatro modelos para os primeiros 1000 resultados retornados. Cada estatística representa a média para
as 30 consultas naquele modelo. O modelo que alcançou a melhor \textit{Mean Average Precision} 
foi o VEC+ANCHOR, no entanto, o modelo COMPLETE alcançou um NDCG maior. O NDCG é uma estatística mais completa, pois mede graus de relevância. 
Além disso, em uma consulta na \textit{web} os usuários estão mais interessados nos primeiros resultados da busca, portanto,
o NDCG dos primeiros 5 documentos mede bem a capacidade do nosso modelo em sanar a necessidade informacional
dos usuários. Dito isso, o modelo que teve o melhor resultado geral foi o COMPLETE. Cabe ressaltar que o cálculo do NDCG
utilizou apenas as avaliações feitas pelos alunos, novas avaliações não foram feitas para documentos que não haviam
sido ranqueados no esforço conjunto.

O \textit{page rank} sozinho acabou piorando os resultados do modelo vetorial. Isso se deve provavelmente à 
algumas particularidades da nossa coleção e à forma como é feita a interpolação entre os modelos.
O \textit{page rank} é uma estatística que ajuda bastante em consultas navegacionais como "vivo" e "uol", no entanto, a nossa
coleção não possui as páginas "vivo.com.br" e "uol.com.br", então páginas com baixa relevância acabam sendo classificadas como
\textit{hubs}. Além disso, a interpolação linear entre o modelo vetorial e o \textit{page rank} acaba dando um peso muito grande
para o \textit{page rank} em todas as consultas.

A nossa máquina de busca não indexa termos com menos de 3 caracteres, números ou \textit{stop words}, portanto, consultas
como "g1" não retornaram nenhum resultado. As consultas "o que é crush" e "por que o feijão está caro" não tiveram documentos
relevantes classificados, portanto, sua análise fica prejudicada. E, a maior parte das consultas tiveram um número muito baixo
de documentos relevantes classificados.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{query_como_fazer_crepioca.png}
\caption{Como fazer crepioca} 
\label{fig:crepioca}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{query_banco_brasil.png}
\caption{Banco brasil} 
\label{fig:banco_brasil}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{query_folha.png}
\caption{Folha} 
\label{fig:folha}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{query_sisu.png}
\caption{Sisu} 
\label{fig:sisu}
\end{figure}

Os gráficos de precisão e revocação construídos utilizam a precisão interpolada (iprec). Ou seja, a precisão no nível de
revocação $ r_j $ se dá pela expressão:

\[
P(r_j) = max_{\forall r : r_j \leq r} P(r)
\]

Logo, a precisão dos níveis de revocação é sempre não crescente.

Ignorando as excessões descritas anteriormente, vamos analisar as duas consultas que tiveram o pior desempenho e as
duas consultas que tiveram o melhor desempenho no modelo COMPLETE com base no NDCG\_5. As piores consultas foram "banco brasil",
com NDCG 0.22, e "como fazer crepioca" com NDCG 0.20 e, as melhores consultas foram "sisu", com NDCG 1.00, e "folha", com
NDCG 0.85. Além dos motivos listados anteriormente, o NDCG é um índice interessante, pois ele mostra quão próximo do melhor
possível é o resultado da máquina de busca, dada a nossa classificação.

A figura \ref{fig:crepioca} mostra o gráfico de precisão e revocação para a consulta "como fazer crepioca". No caso dessa consulta,
podemos observar que o modelo vetorial alcança bons resultados, no entanto, a incorporação dos \textit{anchor texts} piora o 
resultado geral. Isso ocorre porque as páginas com o melhor \textit{score} são apontadas por \textit{links} com
a palavra "fazer" e não "crepioca", que é o termo mais relevante para expressar a intenção do usuário. No caso do \textit{anchor text},
a forma como o algoritmo foi implementado não leva em consideração o TF-IDF dos termos, algo que agora se mostra um problema.

A figura \ref{fig:banco_brasil} mostra o gráfico de precisão e revocação para a consulta "banco brasil". Observe que o NDCG\_5 é maior
que zero apesar da precisão ser zero em todos os níveis de revocação. Isso ocorre porque documentos classificados com \textit{score} 
menor que 3 são considerados irrelevantes, no entanto, eles entram no cálculo do NDCG\_5.
Como no caso da consulta "como fazer crepioca", a incorporação dos \textit{anchor texts} piorou o resultado em comparação ao modelo vetorial. No entanto,
nesse caso isso se dá provavelmente pelas páginas apontadas pelo termo "brasil" cujo conteúdo não é diretamente relacionado com
o "banco do brasil". Dito isso, mesmo o modelo vetorial não alcança resultados muito bons. No caso dessa consulta, o \textit{overhead}
de significados das palavras "banco" e "brasil" é muito grande, então, os resultados acabam retornando páginas de outros bancos ou páginas
brasileiras que mencionam algum banco.

As figuras \ref{fig:folha} e \ref{fig:sisu} mostram os gráficos de precisão e revocação para as melhores consultas. Nesse caso,
a incorporação dos \textit{anchor texts} foi fundamental para garantir bons resultados. No caso da consulta "folha", a intenção
do usuário é clara e, praticamente qualquer página no \textit{site} da Folha de São Paulo vai ter alguma relevância para a consulta.
No caso da consulta "sisu", por ser um termo relativamente raro, a maior parte das páginas retornadas tende a ter conteúdo próximo
da necessidade informacional do usuário. Além disso, ao utilizar a informação dos \textit{anchor texts} para classificar os resultados 
acabamos colocando as páginas cujo conteúdo é exclusivamente sobre o "sisu" em posições mais altas do \textit{rank}.
Cabe ressaltar que o bom desempenho
da máquina de busca nessas consultas também deriva da forma como os documentos relevantes foram escolhidos. Como os próprios 
documentos ranqueados pela máquina de busca que estamos estudando faziam parte dos documentos avaliados por humanos, é natural que a taxa de acerto 
seja boa. Ou, ao menos bem melhor do que se a coleção inteira tivesse sido avaliada.

\section{Conclusão}

O objetivo do trabalho prático 3 era implementar um sistema de consultas e uma interface gráfica. O resultado
está disponível em visconde.latin.dcc.ufmg.br:8080 (só funciona na porta 8080). Os sinais do
\textit{anchor text} foram muito valiosos para melhorar o resultados das buscas, no entanto,
o \textit{page rank} mostrou um desempenho aquém do desejável. Dadas as limitações do método de avaliação humana 
utilizado, os resultados gerais dos experimentos foram positivos.

\section{Apêndice A}

O apêndice contém os gráficos de precisão interpolada e revocação para todas as consultas, utilizando a mesma
legenda dos gráficos apresentados no corpo do texto. Como o modelo COMPLETE dá um peso muito maior para o \textit{anchor text}
do que para o \textit{page rank}, a curva do modelo VEC+ANCHOR acaba ficando escondida pelo modelo COMPLETE em parte dos gráficos.
Os gráficos que são linhas horizontais normalmente contém muito poucos documentos relevantes, então a revocação 100\% é alcançada
rapidamente e, devido ao critério da precisão interpolada, o maior nível de precisão é propagado para os níveis de revocação anteriores.

Tome o caso da consulta "belo horizonte" que só contém 2 documentos relevantes. Nesse caso, o nível de revocação 100\% é alcançado antes
do décimo documento para o modelo COMPLETE.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{queries_01_to_06.png}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{queries_07_to_12.png}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{queries_13_to_18.png}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{queries_19_to_24.png}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{queries_25_to_30.png}
\end{figure}

\end{document}
